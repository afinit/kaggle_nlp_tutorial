{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import describe\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/labeledTrainData.tsv\", header = 0, delimiter = \"\\t\", quoting=3, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train = pd.read_csv(\"../data/unlabeledTrainData.tsv\", header = 0, \n",
    "                              delimiter = '\\t', quoting=3, encoding = 'utf-8')\n",
    "unlabeled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../data/testData.tsv\", header = 0, delimiter = '\\t', quoting=3, encoding = 'utf-8')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( raw_review, remove_stopwords=False ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # 4. In Python, searching a set is much faster than searching\n",
    "        #   a list, so convert the stop words to a set\n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        # \n",
    "        # 5. Remove stop words\n",
    "        words = [w for w in words if not w in stops]   \n",
    "    \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    ## split review into sentences with NLTK\n",
    "    ## 1. split into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    ## 2. loop over sentences\n",
    "    sentences = []\n",
    "    for rs in raw_sentences:\n",
    "        if len(rs) > 0:\n",
    "            sentences.append(review_to_wordlist(rs, remove_stopwords))\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Training Data\n",
      "Parsing Unlabeled Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python2.7/dist-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print(\"Parsing Training Data\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing Unlabeled Training Data\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data for Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "nreviews = train[\"review\"].size\n",
    "\n",
    "for i in range():\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Processing review {}/{}\".format(i,nreviews))\n",
    "    clean_train_reviews.append(review_to_words( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-11 10:14:40,266 : INFO : collecting all words and their counts\n",
      "2017-11-11 10:14:40,268 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-11 10:14:40,398 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-11 10:14:40,492 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2017-11-11 10:14:40,571 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2017-11-11 10:14:40,661 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2017-11-11 10:14:40,762 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2017-11-11 10:14:40,848 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2017-11-11 10:14:40,935 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2017-11-11 10:14:41,023 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2017-11-11 10:14:41,111 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2017-11-11 10:14:41,189 : INFO : PROGRESS: at sentence #100000, processed 2226967 words, keeping 50207 word types\n",
      "2017-11-11 10:14:41,274 : INFO : PROGRESS: at sentence #110000, processed 2446581 words, keeping 52081 word types\n",
      "2017-11-11 10:14:41,362 : INFO : PROGRESS: at sentence #120000, processed 2668776 words, keeping 54119 word types\n",
      "2017-11-11 10:14:41,450 : INFO : PROGRESS: at sentence #130000, processed 2894304 words, keeping 55847 word types\n",
      "2017-11-11 10:14:41,536 : INFO : PROGRESS: at sentence #140000, processed 3107006 words, keeping 57346 word types\n",
      "2017-11-11 10:14:41,619 : INFO : PROGRESS: at sentence #150000, processed 3332628 words, keeping 59055 word types\n",
      "2017-11-11 10:14:41,708 : INFO : PROGRESS: at sentence #160000, processed 3555316 words, keeping 60617 word types\n",
      "2017-11-11 10:14:41,796 : INFO : PROGRESS: at sentence #170000, processed 3778656 words, keeping 62077 word types\n",
      "2017-11-11 10:14:41,883 : INFO : PROGRESS: at sentence #180000, processed 3999237 words, keeping 63496 word types\n",
      "2017-11-11 10:14:41,972 : INFO : PROGRESS: at sentence #190000, processed 4224450 words, keeping 64794 word types\n",
      "2017-11-11 10:14:42,061 : INFO : PROGRESS: at sentence #200000, processed 4448604 words, keeping 66087 word types\n",
      "2017-11-11 10:14:42,151 : INFO : PROGRESS: at sentence #210000, processed 4669968 words, keeping 67390 word types\n",
      "2017-11-11 10:14:42,241 : INFO : PROGRESS: at sentence #220000, processed 4894969 words, keeping 68697 word types\n",
      "2017-11-11 10:14:42,329 : INFO : PROGRESS: at sentence #230000, processed 5117546 words, keeping 69958 word types\n",
      "2017-11-11 10:14:42,419 : INFO : PROGRESS: at sentence #240000, processed 5345051 words, keeping 71167 word types\n",
      "2017-11-11 10:14:42,505 : INFO : PROGRESS: at sentence #250000, processed 5559166 words, keeping 72351 word types\n",
      "2017-11-11 10:14:42,590 : INFO : PROGRESS: at sentence #260000, processed 5779147 words, keeping 73478 word types\n",
      "2017-11-11 10:14:42,680 : INFO : PROGRESS: at sentence #270000, processed 6000436 words, keeping 74767 word types\n",
      "2017-11-11 10:14:42,765 : INFO : PROGRESS: at sentence #280000, processed 6226315 words, keeping 76369 word types\n",
      "2017-11-11 10:14:42,853 : INFO : PROGRESS: at sentence #290000, processed 6449475 words, keeping 77839 word types\n",
      "2017-11-11 10:14:42,942 : INFO : PROGRESS: at sentence #300000, processed 6674078 words, keeping 79171 word types\n",
      "2017-11-11 10:14:43,032 : INFO : PROGRESS: at sentence #310000, processed 6899392 words, keeping 80480 word types\n",
      "2017-11-11 10:14:43,122 : INFO : PROGRESS: at sentence #320000, processed 7124279 words, keeping 81808 word types\n",
      "2017-11-11 10:14:43,213 : INFO : PROGRESS: at sentence #330000, processed 7346022 words, keeping 83030 word types\n",
      "2017-11-11 10:14:43,304 : INFO : PROGRESS: at sentence #340000, processed 7575534 words, keeping 84280 word types\n",
      "2017-11-11 10:14:43,392 : INFO : PROGRESS: at sentence #350000, processed 7798804 words, keeping 85425 word types\n",
      "2017-11-11 10:14:43,480 : INFO : PROGRESS: at sentence #360000, processed 8019467 words, keeping 86596 word types\n",
      "2017-11-11 10:14:43,574 : INFO : PROGRESS: at sentence #370000, processed 8246659 words, keeping 87708 word types\n",
      "2017-11-11 10:14:43,662 : INFO : PROGRESS: at sentence #380000, processed 8471806 words, keeping 88878 word types\n",
      "2017-11-11 10:14:43,755 : INFO : PROGRESS: at sentence #390000, processed 8701556 words, keeping 89907 word types\n",
      "2017-11-11 10:14:43,844 : INFO : PROGRESS: at sentence #400000, processed 8924505 words, keeping 90916 word types\n",
      "2017-11-11 10:14:43,933 : INFO : PROGRESS: at sentence #410000, processed 9145855 words, keeping 91880 word types\n",
      "2017-11-11 10:14:44,022 : INFO : PROGRESS: at sentence #420000, processed 9366935 words, keeping 92912 word types\n",
      "2017-11-11 10:14:44,112 : INFO : PROGRESS: at sentence #430000, processed 9594472 words, keeping 93932 word types\n",
      "2017-11-11 10:14:44,205 : INFO : PROGRESS: at sentence #440000, processed 9821225 words, keeping 94906 word types\n",
      "2017-11-11 10:14:44,295 : INFO : PROGRESS: at sentence #450000, processed 10044987 words, keeping 96036 word types\n",
      "2017-11-11 10:14:44,388 : INFO : PROGRESS: at sentence #460000, processed 10277747 words, keeping 97088 word types\n",
      "2017-11-11 10:14:44,478 : INFO : PROGRESS: at sentence #470000, processed 10505672 words, keeping 97933 word types\n",
      "2017-11-11 10:14:44,567 : INFO : PROGRESS: at sentence #480000, processed 10726056 words, keeping 98862 word types\n",
      "2017-11-11 10:14:44,657 : INFO : PROGRESS: at sentence #490000, processed 10952800 words, keeping 99871 word types\n",
      "2017-11-11 10:14:44,748 : INFO : PROGRESS: at sentence #500000, processed 11174456 words, keeping 100765 word types\n",
      "2017-11-11 10:14:44,838 : INFO : PROGRESS: at sentence #510000, processed 11399731 words, keeping 101699 word types\n",
      "2017-11-11 10:14:44,926 : INFO : PROGRESS: at sentence #520000, processed 11623082 words, keeping 102598 word types\n",
      "2017-11-11 10:14:45,005 : INFO : PROGRESS: at sentence #530000, processed 11847480 words, keeping 103400 word types\n",
      "2017-11-11 10:14:45,094 : INFO : PROGRESS: at sentence #540000, processed 12072095 words, keeping 104265 word types\n",
      "2017-11-11 10:14:45,185 : INFO : PROGRESS: at sentence #550000, processed 12297646 words, keeping 105133 word types\n",
      "2017-11-11 10:14:45,276 : INFO : PROGRESS: at sentence #560000, processed 12518936 words, keeping 105997 word types\n",
      "2017-11-11 10:14:45,353 : INFO : PROGRESS: at sentence #570000, processed 12748083 words, keeping 106787 word types\n",
      "2017-11-11 10:14:45,442 : INFO : PROGRESS: at sentence #580000, processed 12969579 words, keeping 107665 word types\n",
      "2017-11-11 10:14:45,520 : INFO : PROGRESS: at sentence #590000, processed 13195104 words, keeping 108501 word types\n",
      "2017-11-11 10:14:45,608 : INFO : PROGRESS: at sentence #600000, processed 13417302 words, keeping 109218 word types\n",
      "2017-11-11 10:14:45,697 : INFO : PROGRESS: at sentence #610000, processed 13638325 words, keeping 110092 word types\n",
      "2017-11-11 10:14:45,789 : INFO : PROGRESS: at sentence #620000, processed 13864650 words, keeping 110837 word types\n",
      "2017-11-11 10:14:45,875 : INFO : PROGRESS: at sentence #630000, processed 14088936 words, keeping 111610 word types\n",
      "2017-11-11 10:14:45,943 : INFO : PROGRESS: at sentence #640000, processed 14309719 words, keeping 112416 word types\n",
      "2017-11-11 10:14:46,032 : INFO : PROGRESS: at sentence #650000, processed 14535475 words, keeping 113196 word types\n",
      "2017-11-11 10:14:46,120 : INFO : PROGRESS: at sentence #660000, processed 14758265 words, keeping 113945 word types\n",
      "2017-11-11 10:14:46,208 : INFO : PROGRESS: at sentence #670000, processed 14981658 words, keeping 114643 word types\n",
      "2017-11-11 10:14:46,296 : INFO : PROGRESS: at sentence #680000, processed 15206490 words, keeping 115354 word types\n",
      "2017-11-11 10:14:46,387 : INFO : PROGRESS: at sentence #690000, processed 15428683 words, keeping 116131 word types\n",
      "2017-11-11 10:14:46,477 : INFO : PROGRESS: at sentence #700000, processed 15657389 words, keeping 116943 word types\n",
      "2017-11-11 10:14:46,560 : INFO : PROGRESS: at sentence #710000, processed 15880378 words, keeping 117596 word types\n",
      "2017-11-11 10:14:46,648 : INFO : PROGRESS: at sentence #720000, processed 16105665 words, keeping 118221 word types\n",
      "2017-11-11 10:14:46,737 : INFO : PROGRESS: at sentence #730000, processed 16332046 words, keeping 118954 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-11 10:14:46,823 : INFO : PROGRESS: at sentence #740000, processed 16553079 words, keeping 119668 word types\n",
      "2017-11-11 10:14:46,910 : INFO : PROGRESS: at sentence #750000, processed 16771406 words, keeping 120295 word types\n",
      "2017-11-11 10:14:46,995 : INFO : PROGRESS: at sentence #760000, processed 16990810 words, keeping 120930 word types\n",
      "2017-11-11 10:14:47,084 : INFO : PROGRESS: at sentence #770000, processed 17217947 words, keeping 121703 word types\n",
      "2017-11-11 10:14:47,175 : INFO : PROGRESS: at sentence #780000, processed 17448093 words, keeping 122402 word types\n",
      "2017-11-11 10:14:47,265 : INFO : PROGRESS: at sentence #790000, processed 17675169 words, keeping 123066 word types\n",
      "2017-11-11 10:14:47,315 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 795538 sentences\n",
      "2017-11-11 10:14:47,315 : INFO : Loading a fresh vocabulary\n",
      "2017-11-11 10:14:47,432 : INFO : min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2017-11-11 10:14:47,432 : INFO : min_count=40 leaves 17239125 word corpus (96% of original 17798270, drops 559145)\n",
      "2017-11-11 10:14:47,509 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2017-11-11 10:14:47,514 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-11-11 10:14:47,515 : INFO : downsampling leaves estimated 12749798 word corpus (74.0% of prior 17239125)\n",
      "2017-11-11 10:14:47,516 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2017-11-11 10:14:47,575 : INFO : resetting layer weights\n",
      "2017-11-11 10:14:47,915 : INFO : training model with 8 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-11 10:14:48,936 : INFO : PROGRESS: at 1.10% examples, 689650 words/s, in_qsize 16, out_qsize 1\n",
      "2017-11-11 10:14:49,942 : INFO : PROGRESS: at 2.35% examples, 735250 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:14:50,952 : INFO : PROGRESS: at 3.59% examples, 749819 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:14:51,955 : INFO : PROGRESS: at 4.88% examples, 765922 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:14:52,976 : INFO : PROGRESS: at 6.12% examples, 770779 words/s, in_qsize 14, out_qsize 2\n",
      "2017-11-11 10:14:53,974 : INFO : PROGRESS: at 7.36% examples, 770092 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:14:54,977 : INFO : PROGRESS: at 8.65% examples, 778132 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:14:55,983 : INFO : PROGRESS: at 9.87% examples, 777848 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:14:56,996 : INFO : PROGRESS: at 11.12% examples, 779540 words/s, in_qsize 16, out_qsize 1\n",
      "2017-11-11 10:14:58,021 : INFO : PROGRESS: at 12.38% examples, 781588 words/s, in_qsize 14, out_qsize 2\n",
      "2017-11-11 10:14:59,024 : INFO : PROGRESS: at 13.65% examples, 783041 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:00,043 : INFO : PROGRESS: at 14.86% examples, 782127 words/s, in_qsize 13, out_qsize 2\n",
      "2017-11-11 10:15:01,033 : INFO : PROGRESS: at 16.12% examples, 783041 words/s, in_qsize 16, out_qsize 1\n",
      "2017-11-11 10:15:02,041 : INFO : PROGRESS: at 17.34% examples, 782740 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:03,043 : INFO : PROGRESS: at 18.59% examples, 783839 words/s, in_qsize 16, out_qsize 1\n",
      "2017-11-11 10:15:04,040 : INFO : PROGRESS: at 19.83% examples, 784387 words/s, in_qsize 14, out_qsize 1\n",
      "2017-11-11 10:15:05,053 : INFO : PROGRESS: at 21.12% examples, 785868 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:06,069 : INFO : PROGRESS: at 22.37% examples, 785153 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:07,085 : INFO : PROGRESS: at 23.60% examples, 784590 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:15:08,087 : INFO : PROGRESS: at 24.80% examples, 783265 words/s, in_qsize 15, out_qsize 2\n",
      "2017-11-11 10:15:09,085 : INFO : PROGRESS: at 26.07% examples, 784113 words/s, in_qsize 16, out_qsize 2\n",
      "2017-11-11 10:15:10,100 : INFO : PROGRESS: at 27.32% examples, 784339 words/s, in_qsize 16, out_qsize 4\n",
      "2017-11-11 10:15:11,100 : INFO : PROGRESS: at 28.60% examples, 785352 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:12,103 : INFO : PROGRESS: at 29.78% examples, 784134 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:13,121 : INFO : PROGRESS: at 31.00% examples, 783722 words/s, in_qsize 14, out_qsize 1\n",
      "2017-11-11 10:15:14,114 : INFO : PROGRESS: at 32.23% examples, 784017 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:15:15,130 : INFO : PROGRESS: at 33.49% examples, 784415 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:16,137 : INFO : PROGRESS: at 34.74% examples, 784599 words/s, in_qsize 13, out_qsize 1\n",
      "2017-11-11 10:15:17,168 : INFO : PROGRESS: at 35.98% examples, 784756 words/s, in_qsize 15, out_qsize 3\n",
      "2017-11-11 10:15:18,173 : INFO : PROGRESS: at 37.25% examples, 784720 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:19,189 : INFO : PROGRESS: at 38.48% examples, 784359 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:20,205 : INFO : PROGRESS: at 39.74% examples, 784738 words/s, in_qsize 14, out_qsize 1\n",
      "2017-11-11 10:15:21,206 : INFO : PROGRESS: at 40.94% examples, 784102 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:22,222 : INFO : PROGRESS: at 42.24% examples, 784844 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:23,238 : INFO : PROGRESS: at 43.53% examples, 785344 words/s, in_qsize 12, out_qsize 2\n",
      "2017-11-11 10:15:24,237 : INFO : PROGRESS: at 44.78% examples, 785470 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:25,246 : INFO : PROGRESS: at 46.04% examples, 786049 words/s, in_qsize 16, out_qsize 1\n",
      "2017-11-11 10:15:26,249 : INFO : PROGRESS: at 47.31% examples, 786172 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:27,254 : INFO : PROGRESS: at 48.56% examples, 786524 words/s, in_qsize 13, out_qsize 3\n",
      "2017-11-11 10:15:28,263 : INFO : PROGRESS: at 49.80% examples, 786475 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:15:29,266 : INFO : PROGRESS: at 51.06% examples, 786943 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:30,272 : INFO : PROGRESS: at 52.31% examples, 787124 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:15:31,277 : INFO : PROGRESS: at 53.54% examples, 787028 words/s, in_qsize 14, out_qsize 1\n",
      "2017-11-11 10:15:32,286 : INFO : PROGRESS: at 54.79% examples, 787192 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:33,300 : INFO : PROGRESS: at 56.03% examples, 786970 words/s, in_qsize 14, out_qsize 0\n",
      "2017-11-11 10:15:34,313 : INFO : PROGRESS: at 57.30% examples, 787190 words/s, in_qsize 14, out_qsize 1\n",
      "2017-11-11 10:15:35,327 : INFO : PROGRESS: at 58.57% examples, 787571 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:36,330 : INFO : PROGRESS: at 59.80% examples, 787483 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:37,339 : INFO : PROGRESS: at 61.06% examples, 787834 words/s, in_qsize 13, out_qsize 0\n",
      "2017-11-11 10:15:38,345 : INFO : PROGRESS: at 62.31% examples, 787652 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:39,365 : INFO : PROGRESS: at 63.59% examples, 787886 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:40,372 : INFO : PROGRESS: at 64.89% examples, 788314 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:41,376 : INFO : PROGRESS: at 66.10% examples, 787952 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:15:42,391 : INFO : PROGRESS: at 67.37% examples, 787949 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:43,402 : INFO : PROGRESS: at 68.58% examples, 787642 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:44,419 : INFO : PROGRESS: at 69.89% examples, 788266 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:45,430 : INFO : PROGRESS: at 71.14% examples, 788362 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:46,434 : INFO : PROGRESS: at 72.34% examples, 788007 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:47,459 : INFO : PROGRESS: at 73.61% examples, 788026 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:48,477 : INFO : PROGRESS: at 74.86% examples, 788156 words/s, in_qsize 14, out_qsize 3\n",
      "2017-11-11 10:15:49,470 : INFO : PROGRESS: at 76.08% examples, 787982 words/s, in_qsize 16, out_qsize 2\n",
      "2017-11-11 10:15:50,478 : INFO : PROGRESS: at 77.36% examples, 788346 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:15:51,487 : INFO : PROGRESS: at 78.61% examples, 788402 words/s, in_qsize 15, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-11 10:15:52,493 : INFO : PROGRESS: at 79.85% examples, 788386 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:53,510 : INFO : PROGRESS: at 81.11% examples, 788374 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:54,517 : INFO : PROGRESS: at 82.34% examples, 788151 words/s, in_qsize 15, out_qsize 3\n",
      "2017-11-11 10:15:55,523 : INFO : PROGRESS: at 83.61% examples, 788180 words/s, in_qsize 14, out_qsize 1\n",
      "2017-11-11 10:15:56,531 : INFO : PROGRESS: at 84.86% examples, 788162 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:15:57,533 : INFO : PROGRESS: at 86.14% examples, 788517 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:58,540 : INFO : PROGRESS: at 87.42% examples, 788823 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:15:59,540 : INFO : PROGRESS: at 88.65% examples, 788765 words/s, in_qsize 16, out_qsize 2\n",
      "2017-11-11 10:16:00,541 : INFO : PROGRESS: at 89.88% examples, 788733 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:16:01,550 : INFO : PROGRESS: at 91.11% examples, 788598 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:16:02,562 : INFO : PROGRESS: at 92.33% examples, 788460 words/s, in_qsize 14, out_qsize 2\n",
      "2017-11-11 10:16:03,574 : INFO : PROGRESS: at 93.58% examples, 788488 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:16:04,575 : INFO : PROGRESS: at 94.85% examples, 788703 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:16:05,576 : INFO : PROGRESS: at 96.12% examples, 788945 words/s, in_qsize 16, out_qsize 0\n",
      "2017-11-11 10:16:06,590 : INFO : PROGRESS: at 97.35% examples, 788774 words/s, in_qsize 15, out_qsize 0\n",
      "2017-11-11 10:16:07,614 : INFO : PROGRESS: at 98.61% examples, 788864 words/s, in_qsize 15, out_qsize 1\n",
      "2017-11-11 10:16:08,616 : INFO : PROGRESS: at 99.88% examples, 789057 words/s, in_qsize 9, out_qsize 3\n",
      "2017-11-11 10:16:08,635 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-11-11 10:16:08,636 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-11-11 10:16:08,637 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-11-11 10:16:08,648 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-11-11 10:16:08,650 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-11 10:16:08,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-11 10:16:08,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-11 10:16:08,674 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-11 10:16:08,675 : INFO : training on 88991350 raw words (63747751 effective words) took 80.8s, 789393 effective words/s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Word2Vec Model...\")\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          workers = num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count = min_word_count,\n",
    "                          window = context,\n",
    "                          sample = downsampling)\n",
    "\n",
    "## For memory efficiency if no plans to further train..\n",
    "# model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-11 10:17:01,919 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-11-11 10:17:01,921 : INFO : not storing attribute syn0norm\n",
      "2017-11-11 10:17:01,923 : INFO : not storing attribute cum_table\n",
      "2017-11-11 10:17:02,145 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### playing with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-11 10:17:45,770 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.6210911870002747),\n",
       " (u'lady', 0.5788845419883728),\n",
       " (u'lad', 0.5675628185272217),\n",
       " (u'monk', 0.5514678955078125),\n",
       " (u'guy', 0.523241400718689),\n",
       " (u'men', 0.5219612121582031),\n",
       " (u'millionaire', 0.5186538696289062),\n",
       " (u'farmer', 0.5138608813285828),\n",
       " (u'doctor', 0.5085681676864624),\n",
       " (u'soldier', 0.5004254579544067)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'princess', 0.6849184632301331),\n",
       " (u'stepmother', 0.6160538792610168),\n",
       " (u'bride', 0.6057089567184448),\n",
       " (u'belle', 0.6033458113670349),\n",
       " (u'diana', 0.5968631505966187),\n",
       " (u'victoria', 0.5925002098083496),\n",
       " (u'ling', 0.5852207541465759),\n",
       " (u'goddess', 0.5832862257957458),\n",
       " (u'mistress', 0.5803508758544922),\n",
       " (u'maria', 0.580348014831543)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.7664473056793213),\n",
       " (u'atrocious', 0.7393538355827332),\n",
       " (u'horrible', 0.7315835356712341),\n",
       " (u'abysmal', 0.728158175945282),\n",
       " (u'dreadful', 0.7171405553817749),\n",
       " (u'horrendous', 0.6993017196655273),\n",
       " (u'appalling', 0.6877903938293457),\n",
       " (u'horrid', 0.6677767634391785),\n",
       " (u'lousy', 0.6274197697639465),\n",
       " (u'amateurish', 0.6212085485458374)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Words To Paragraphs, Attempt 1: Vector Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Model (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-12 16:02:08,369 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2017-11-12 16:02:08,469 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2017-11-12 16:02:08,470 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-12 16:02:08,471 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-12 16:02:08,472 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "#model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "model0 = Word2Vec.load(\"300features_40minwords_10context\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-12 16:03:52,787 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model0.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.syn0_lockf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn1neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Vectors for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avgFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    idx2word = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in idx2word:\n",
    "            nwords += 1\n",
    "            featureVec += model[word]\n",
    "            \n",
    "    featureVec /= nwords\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    \n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features), dtype = \"float32\" )\n",
    "    \n",
    "    for r in reviews:\n",
    "        if counter % 1000 == 0:\n",
    "            print(\"Processing review {} of {}\".format(counter, len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = avgFeatureVec(r, model, num_features)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing review 0 of 25000\n",
      "Processing review 1000 of 25000\n",
      "Processing review 2000 of 25000\n",
      "Processing review 3000 of 25000\n",
      "Processing review 4000 of 25000\n",
      "Processing review 5000 of 25000\n",
      "Processing review 6000 of 25000\n",
      "Processing review 7000 of 25000\n",
      "Processing review 8000 of 25000\n",
      "Processing review 9000 of 25000\n",
      "Processing review 10000 of 25000\n",
      "Processing review 11000 of 25000\n",
      "Processing review 12000 of 25000\n",
      "Processing review 13000 of 25000\n",
      "Processing review 14000 of 25000\n",
      "Processing review 15000 of 25000\n",
      "Processing review 16000 of 25000\n",
      "Processing review 17000 of 25000\n",
      "Processing review 18000 of 25000\n",
      "Processing review 19000 of 25000\n",
      "Processing review 20000 of 25000\n",
      "Processing review 21000 of 25000\n",
      "Processing review 22000 of 25000\n",
      "Processing review 23000 of 25000\n",
      "Processing review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Processing review 0 of 25000\n",
      "Processing review 1000 of 25000\n",
      "Processing review 2000 of 25000\n",
      "Processing review 3000 of 25000\n",
      "Processing review 4000 of 25000\n",
      "Processing review 5000 of 25000\n",
      "Processing review 6000 of 25000\n",
      "Processing review 7000 of 25000\n",
      "Processing review 8000 of 25000\n",
      "Processing review 9000 of 25000\n",
      "Processing review 10000 of 25000\n",
      "Processing review 11000 of 25000\n",
      "Processing review 12000 of 25000\n",
      "Processing review 13000 of 25000\n",
      "Processing review 14000 of 25000\n",
      "Processing review 15000 of 25000\n",
      "Processing review 16000 of 25000\n",
      "Processing review 17000 of 25000\n",
      "Processing review 18000 of 25000\n",
      "Processing review 19000 of 25000\n",
      "Processing review 20000 of 25000\n",
      "Processing review 21000 of 25000\n",
      "Processing review 22000 of 25000\n",
      "Processing review 23000 of 25000\n",
      "Processing review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# ****************************************\n",
    "# Calculate avg feature vectors for both training and test sets\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier( n_estimators = 100, oob_score=True)\n",
    "\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"])\n",
    "\n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame({\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82235999999999998"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Words to Paragraphs, Attempt 2: Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1649\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv.syn0\n",
    "num_clusters = word_vectors.shape[0] / 10\n",
    "print(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 443.268747091\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(\"Time Elapsed: {}\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16490"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peaking at a couple clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_df = pd.DataFrame({\"wrd\":model.wv.index2word, \"idx\":idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>wrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1023</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1064</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1293</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx  wrd\n",
       "0   142  the\n",
       "1  1023  and\n",
       "2  1064    a\n",
       "3  1293   of\n",
       "4   731   to"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_centroid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142, u'the'],\n",
       "       [1023, u'and'],\n",
       "       [1064, u'a'],\n",
       "       [1293, u'of'],\n",
       "       [731, u'to']], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_centroid_df.values[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster0\n",
      "[u'within']\n",
      "Cluster1\n",
      "[u'minus' u'btw' u'mega' u'programme' u'clerks' u'newer' u'caper'\n",
      " u'paycheck' u'novelty' u'skit' u'imo' u'gentlemen' u'cheers' u'recipe'\n",
      " u'hallmark' u'sleeper' u'sonic' u'bio' u'watcher' u'shocker' u'pixar'\n",
      " u'bogus' u'noting' u'mimic' u'queer' u'peanuts' u'thirteen' u'csi'\n",
      " u'hound' u'demographic' u'filipino' u'congratulations' u'crop' u'pic'\n",
      " u'muppets' u'voyager' u'hrs' u'mockumentary' u'draft' u'marginally'\n",
      " u'copying' u'splash' u'slew' u'smallville' u'keystone' u'consisted'\n",
      " u'continuation' u'deadwood' u'pauly' u'telly' u'cliffhanger' u'blackadder'\n",
      " u'ff' u'sans' u'slot' u'hanna' u'monstrosity' u'scoring' u'clunker'\n",
      " u'maury' u'futurama' u'relaxing' u'newest' u'preferably' u'kindergarten'\n",
      " u'contender' u'math' u'wb' u'pinhead' u'xxx' u'escapist' u'fondly'\n",
      " u'yankees' u'vanilla' u'ranked' u'indy' u'tagline' u'beckham' u'viva'\n",
      " u'heavens' u'miracles' u'gong' u'addams' u'groove' u'watchers'\n",
      " u'bruckheimer' u'contestant' u'diversion' u'grader' u'thou' u'timer'\n",
      " u'outtakes' u'recap' u'yrs' u'shudder' u'kickboxer' u'replay' u'scrap'\n",
      " u'housewives' u'stooge' u'soo' u'jackass' u'sasquatch' u'flops'\n",
      " u'armageddon' u'sexploitation' u'thunderbirds' u'salute' u'crossword'\n",
      " u'deja' u'spoofing' u'cancel' u'dreamworks' u'dino' u'memento'\n",
      " u'fashionable' u'tuesday' u'ppv' u'confessions' u'commercially'\n",
      " u'separately' u'enthusiasts' u'banner' u'hippy' u'brightest' u'gamble'\n",
      " u'bsg' u'chiller' u'stripes' u'ultimatum' u'leprechaun' u'airwolf'\n",
      " u'dumplings' u'magnolia' u'wwf' u'explorers' u'kiddies' u'trinity'\n",
      " u'remaking' u'ncis' u'extravaganza' u'sesame' u'fencing' u'iceberg'\n",
      " u'junkies' u'babylon' u'tornado' u'criteria' u'poltergeist' u'dross'\n",
      " u'unquestionably' u'trainspotting' u'cradle' u'wayans' u'definately'\n",
      " u'rankin' u'kin' u'mi' u'rerun' u'cannonball' u'cinemax' u'promotional'\n",
      " u'scrappy' u'relic' u'bebop' u'showgirls' u'import' u'bitterly' u'tenth'\n",
      " u'das' u'crud' u'wholeheartedly' u'theatrically' u'champions' u'cr'\n",
      " u'hounds' u'stardust' u'apples' u'finnish' u'jeez' u'bonanza' u'packaging'\n",
      " u'exec' u'haters' u'buckaroo' u'headlines' u'gaming' u'mk' u'makings'\n",
      " u'quickie' u'stint' u'bots' u'mercury' u'debuted' u'template' u'ahem'\n",
      " u'dtv' u'retread' u'whomever' u'soaps' u'tuning' u'specialized' u'blurb'\n",
      " u'advertisements' u'tbs' u'potboiler' u'micro' u'copyright' u'recordings'\n",
      " u'swordplay' u'crews' u'cbc' u'hobgoblins' u'comet' u'hobbit' u'burger'\n",
      " u'lottery' u'spongebob' u'aquaman' u'imax' u'andreas' u'es' u'imitations'\n",
      " u'announcement' u'hazzard' u'anchorman' u'completists' u'prequels'\n",
      " u'elfen' u'execs' u'wynorski' u'spotting' u'solaris' u'waster' u'sneakers'\n",
      " u'studded' u'avengers' u'nintendo' u'cw' u'promos' u'skater' u'vol'\n",
      " u'ninth' u'rappers' u'superheroes' u'shriek' u'azumi' u'shortage'\n",
      " u'sooooo' u'prizes' u'kamal' u'vinyl' u'heh' u'cheapest' u'dun' u'eighth'\n",
      " u'rpg' u'aficionados' u'anarchist' u'distribute' u'homages' u'gamers'\n",
      " u'biz' u'greetings' u'lineup' u'bandwagon' u'nah' u'delayed' u'monogram'\n",
      " u'saddles' u'veritable' u'lite' u'mil' u'soundtracks' u'gta' u'detour'\n",
      " u'venue' u'popeye' u'barbera' u'panel' u'caprica' u'requiem' u'ching'\n",
      " u'beta' u'mallrats' u'tex' u'sholay' u'fraction' u'saawariya' u'gidget'\n",
      " u'heaps' u'netherlands' u'mondo' u'trolls' u'malibu' u'starcraft'\n",
      " u'pusher' u'gigli' u'varma' u'franchises' u'farrelly' u'dune' u'cosby'\n",
      " u'actioner' u'nickelodeon' u'bluth' u'nightly' u'vii' u'slackers' u'sony'\n",
      " u'thundercats' u'hr' u'heres' u'boxed' u'eclipse' u'fluke' u'nuff'\n",
      " u'baywatch' u'fastest' u'privates' u'subgenre' u'copycat' u'improv'\n",
      " u'collectors' u'pics' u'ogre' u'swordsman' u'anthem' u'listener'\n",
      " u'independently' u'sleepless' u'countdown' u'fyi' u'chores' u'tashan'\n",
      " u'renamed' u'masala' u'benchmark' u'sa' u'footloose' u'aquatic' u'twister'\n",
      " u'formulas' u'turkeys' u'leagues' u'teaser' u'airs' u'evenings' u'batch'\n",
      " u'screamers' u'improvements' u'catalogue' u'vie' u'overdue' u'pap'\n",
      " u'wasnt' u'retrospective' u'ahh' u'jc' u'headline' u'afi' u'reef'\n",
      " u'scroll' u'circulation' u'rehearsals' u'costumed' u'revisited' u'karaoke'\n",
      " u'programmer' u'ur' u'turbo' u'afore' u'americanized' u'certificate'\n",
      " u'sludge' u'cnn' u'claymation' u'mimzy' u'indoors' u'recommendations'\n",
      " u'pinocchio' u'advent' u'hd' u'blvd' u'muck' u'toons' u'darkwing'\n",
      " u'footnote' u'darwin' u'wrestlemania' u'nineteen' u'prairie' u'glitter'\n",
      " u'superstars' u'mein' u'daisies' u'zatoichi' u'rep' u'electronics'\n",
      " u'decoys' u'dragonball' u'roster' u'pok' u'dribble' u'braindead'\n",
      " u'fangoria' u'hooray' u'bewitched' u'darko' u'bc' u'indies' u'nacho' u'tt'\n",
      " u'knockoff' u'itv' u'tier' u'guffman' u'completist' u'rediscovered'\n",
      " u'hellworld' u'conditioned' u'barman' u'tnt' u'collections' u'uncensored'\n",
      " u'shaq' u'raider' u'lumiere' u'showings' u'darkman' u'rts' u'ghai'\n",
      " u'captions' u'eagles' u'whopping' u'mcg' u'pandora' u'hefty' u'instalment'\n",
      " u'frasier' u'decorations' u'lookout']\n",
      "Cluster2\n",
      "[u'tough' u'mad' u'nasty' u'hunter' u'headed' u'sleazy' u'sidekick'\n",
      " u'menacing' u'ruthless' u'bumbling' u'mouthed' u'slimy']\n",
      "Cluster3\n",
      "[u'ford' u'wayne' u'waters' u'woo' u'carpenter' u'hughes' u'lenny'\n",
      " u'carradine' u'cusack' u'travolta' u'cassavetes' u'huston' u'garfield'\n",
      " u'ritter' u'payne' u'goodman' u'lennon' u'abraham' u'malkovich' u'saxon'\n",
      " u'doe' u'gielgud' u'lithgow' u'cleese']\n",
      "Cluster4\n",
      "[u'addition' u'vehicle' u'prime' u'welcome' u'latest' u'tribute' u'glory'\n",
      " u'undoubtedly' u'celebrity' u'alongside' u'equivalent' u'roots'\n",
      " u'greatness' u'showcase' u'popularity' u'imitation' u'arguably'\n",
      " u'testament' u'legacy' u'heights' u'caliber' u'predecessor' u'deserving'\n",
      " u'nod' u'disgrace' u'idol' u'globe' u'stardom' u'contribution' u'demille'\n",
      " u'comeback' u'crown' u'entirety' u'decline' u'celebrated' u'comparable'\n",
      " u'superstar' u'groundbreaking' u'rko' u'milestone' u'excellence'\n",
      " u'forties' u'staple' u'filmography' u'landmark' u'earliest' u'nineties'\n",
      " u'predecessors' u'revival' u'breakthrough' u'achievements' u'warners'\n",
      " u'surpasses' u'restoration' u'output' u'ilk' u'heyday' u'contemporaries']\n",
      "Cluster5\n",
      "[u'powers' u'soldier' u'priest' u'training' u'weapon' u'mafia' u'fighter'\n",
      " u'enemies' u'warrior' u'prisoner' u'slave' u'tribe' u'rebel' u'squad'\n",
      " u'thieves' u'clan']\n",
      "Cluster6\n",
      "[u'causes' u'suggests' u'spell' u'demands' u'realizing' u'promises'\n",
      " u'witnesses' u'chooses' u'rises' u'mentions' u'performs' u'refers'\n",
      " u'survives' u'drove' u'insists' u'eats' u'replies' u'hides' u'crosses'\n",
      " u'admits' u'commits' u'cries' u'sleeps' u'begs' u'notices' u'sells'\n",
      " u'confronts' u'realises' u'considers' u'assumes' u'informs' u'warns'\n",
      " u'forgets' u'ra' u'responds' u'kidnaps' u'blames' u'prefers' u'yells'\n",
      " u'announces' u'intends' u'investigates' u'encourages' u'vanishes'\n",
      " u'complains' u'pursues' u'dumps' u'searches' u'stalks' u'confesses'\n",
      " u'recognizes' u'cheats' u'listens' u'declares' u'awaits' u'awakens'\n",
      " u'seduces' u'enlists' u'prepares' u'uncovers' u'forgives']\n",
      "Cluster7\n",
      "[u'moves' u'cuts' u'jumps']\n",
      "Cluster8\n",
      "[u'players']\n",
      "Cluster9\n",
      "[u'image' u'protagonist' u'attitude' u'reaction' u'speech' u'obsession'\n",
      " u'persona' u'attraction' u'response' u'ego' u'argument' u'illness'\n",
      " u'behaviour' u'inability' u'conscience']\n"
     ]
    }
   ],
   "source": [
    "for c in range(10):\n",
    "    print(\"Cluster{}\".format(c))\n",
    "    \n",
    "    print(word_centroid_df[word_centroid_df[\"idx\"] == c][\"wrd\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create centroid count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100, oob_score = True)\n",
    "\n",
    "forest.fit(train_centroids, train[\"sentiment\"])\n",
    "\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "output = pd.DataFrame({\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83036\n"
     ]
    }
   ],
   "source": [
    "print(forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
